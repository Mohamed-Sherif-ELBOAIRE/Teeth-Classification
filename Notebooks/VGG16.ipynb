{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4614ef8-cf9a-4821-b451-31b4ff7c6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential , save_model, load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, MaxPooling2D, Dropout, SpatialDropout2D, BatchNormalization, Input,Activation, Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc9be8f-3c1b-45c5-8e4e-d0864b952452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3087 images belonging to 7 classes.\n",
      "Found 1028 images belonging to 7 classes.\n",
      "Found 1028 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "img_height, img_width = 128, 128\n",
    "batch_size=32\n",
    "# Data generators for training, validation, and testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # preprocessing_function=preprocess_input,  # Apply ResNet preprocessing\n",
    "    rescale=1./255, \n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2,  \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.3,  \n",
    "    horizontal_flip=True,\n",
    "    vertical_flip= True ,\n",
    "    fill_mode='nearest' \n",
    ")\n",
    "# No Augmentation applied for both Validation and Testing Datasets\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Training',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # 'categorical' for multi-class classification, one-hot encoded vectors\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Validation',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Testing',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0f96bfa-6680-4c7e-9fcc-20374001e2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_step=test_generator.n//test_generator.batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19b852b7-4c53-4651-b862-bb79277ede48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 0, Layer Name: input_2 , Layer Type: InputLayer\n",
      "Index: 1, Layer Name: block1_conv1 , Layer Type: Conv2D\n",
      "Index: 2, Layer Name: block1_conv2 , Layer Type: Conv2D\n",
      "Index: 3, Layer Name: block1_pool , Layer Type: MaxPooling2D\n",
      "Index: 4, Layer Name: block2_conv1 , Layer Type: Conv2D\n",
      "Index: 5, Layer Name: block2_conv2 , Layer Type: Conv2D\n",
      "Index: 6, Layer Name: block2_pool , Layer Type: MaxPooling2D\n",
      "Index: 7, Layer Name: block3_conv1 , Layer Type: Conv2D\n",
      "Index: 8, Layer Name: block3_conv2 , Layer Type: Conv2D\n",
      "Index: 9, Layer Name: block3_conv3 , Layer Type: Conv2D\n",
      "Index: 10, Layer Name: block3_pool , Layer Type: MaxPooling2D\n",
      "Index: 11, Layer Name: block4_conv1 , Layer Type: Conv2D\n",
      "Index: 12, Layer Name: block4_conv2 , Layer Type: Conv2D\n",
      "Index: 13, Layer Name: block4_conv3 , Layer Type: Conv2D\n",
      "Index: 14, Layer Name: block4_pool , Layer Type: MaxPooling2D\n",
      "Index: 15, Layer Name: block5_conv1 , Layer Type: Conv2D\n",
      "Index: 16, Layer Name: block5_conv2 , Layer Type: Conv2D\n",
      "Index: 17, Layer Name: block5_conv3 , Layer Type: Conv2D\n",
      "Index: 18, Layer Name: block5_pool , Layer Type: MaxPooling2D\n"
     ]
    }
   ],
   "source": [
    "vgg16_pretrained = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "\n",
    "# # Freeze the layers you don't want to train\n",
    "# for layer in vgg16_pretrained.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "#Alternatively, you can selectively unfreeze some layers like this:\n",
    "for idx, layer in enumerate(vgg16_pretrained.layers):\n",
    "    print(f\"Index: {idx}, Layer Name: {layer.name} , Layer Type: {layer.__class__.__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9d6c7e-8e5f-46db-80d1-9e57dbc91d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeze the layers you don't want to train\n",
    "# for layer in vgg16_pretrained.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# Alternatively, you can selectively unfreeze some layers like this:\n",
    "for layer in vgg16_pretrained.layers[:15]:\n",
    "    layer.trainable = False\n",
    "for layer in vgg16_pretrained.layers[15:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb681ff-bac9-421e-a1de-dea436c5283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 4, 4, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               4194816   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,042,631\n",
      "Trainable params: 11,407,367\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build your custom model architecture\n",
    "model = Sequential()\n",
    "\n",
    "# Add the pre-trained VGG16 base model\n",
    "model.add(vgg16_pretrained)\n",
    "\n",
    "# Add custom layers on top of the VGG16 base model\n",
    "model.add(Flatten())  # Flatten the output from the VGG16 model\n",
    "model.add(Dense(512, activation='relu'))  # Add a fully connected layer\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(256, activation='relu'))  # Add another fully connected layer\n",
    "model.add(Dropout(0.5))  # Add another dropout layer\n",
    "model.add(Dense(7, activation='softmax'))  # Final output layer with softmax (assuming 7 classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "113c739b-d899-4650-a2ed-1d2a0386c715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "97/97 [==============================] - 22s 199ms/step - loss: 1.9007 - accuracy: 0.2433 - val_loss: 1.4803 - val_accuracy: 0.4553\n",
      "Epoch 2/100\n",
      "97/97 [==============================] - 17s 171ms/step - loss: 1.5851 - accuracy: 0.3777 - val_loss: 1.3276 - val_accuracy: 0.4796\n",
      "Epoch 3/100\n",
      "97/97 [==============================] - 17s 171ms/step - loss: 1.3909 - accuracy: 0.4717 - val_loss: 1.0806 - val_accuracy: 0.6459\n",
      "Epoch 4/100\n",
      "97/97 [==============================] - 17s 172ms/step - loss: 1.2279 - accuracy: 0.5394 - val_loss: 0.8362 - val_accuracy: 0.7189\n",
      "Epoch 5/100\n",
      "97/97 [==============================] - 17s 172ms/step - loss: 1.0688 - accuracy: 0.6041 - val_loss: 0.7198 - val_accuracy: 0.7442\n",
      "Epoch 6/100\n",
      "97/97 [==============================] - 16s 169ms/step - loss: 1.0088 - accuracy: 0.6359 - val_loss: 0.8798 - val_accuracy: 0.6887\n",
      "Epoch 7/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.8542 - accuracy: 0.7013 - val_loss: 0.6452 - val_accuracy: 0.7733\n",
      "Epoch 8/100\n",
      "97/97 [==============================] - 17s 175ms/step - loss: 0.7808 - accuracy: 0.7376 - val_loss: 0.3702 - val_accuracy: 0.8677\n",
      "Epoch 9/100\n",
      "97/97 [==============================] - 16s 167ms/step - loss: 0.6946 - accuracy: 0.7619 - val_loss: 0.4103 - val_accuracy: 0.8541\n",
      "Epoch 10/100\n",
      "97/97 [==============================] - 17s 169ms/step - loss: 0.6527 - accuracy: 0.7716 - val_loss: 0.2993 - val_accuracy: 0.8998\n",
      "Epoch 11/100\n",
      "97/97 [==============================] - 17s 173ms/step - loss: 0.5179 - accuracy: 0.8290 - val_loss: 0.2527 - val_accuracy: 0.9183\n",
      "Epoch 12/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.5226 - accuracy: 0.8332 - val_loss: 0.2321 - val_accuracy: 0.9309\n",
      "Epoch 13/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.4227 - accuracy: 0.8646 - val_loss: 0.2047 - val_accuracy: 0.9251\n",
      "Epoch 14/100\n",
      "97/97 [==============================] - 17s 169ms/step - loss: 0.3935 - accuracy: 0.8714 - val_loss: 0.2506 - val_accuracy: 0.9086\n",
      "Epoch 15/100\n",
      "97/97 [==============================] - 17s 172ms/step - loss: 0.4164 - accuracy: 0.8617 - val_loss: 0.1531 - val_accuracy: 0.9504\n",
      "Epoch 16/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.3480 - accuracy: 0.8860 - val_loss: 0.2686 - val_accuracy: 0.9115\n",
      "Epoch 17/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.3239 - accuracy: 0.8963 - val_loss: 0.1549 - val_accuracy: 0.9475\n",
      "Epoch 18/100\n",
      "97/97 [==============================] - 17s 172ms/step - loss: 0.2613 - accuracy: 0.9164 - val_loss: 0.1975 - val_accuracy: 0.9251\n",
      "Epoch 19/100\n",
      "97/97 [==============================] - 17s 169ms/step - loss: 0.2773 - accuracy: 0.9167 - val_loss: 0.0934 - val_accuracy: 0.9718\n",
      "Epoch 20/100\n",
      "97/97 [==============================] - 17s 178ms/step - loss: 0.2495 - accuracy: 0.9197 - val_loss: 0.0942 - val_accuracy: 0.9698\n",
      "Epoch 21/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.2394 - accuracy: 0.9252 - val_loss: 0.2033 - val_accuracy: 0.9416\n",
      "Epoch 22/100\n",
      "97/97 [==============================] - 20s 207ms/step - loss: 0.2295 - accuracy: 0.9281 - val_loss: 0.0650 - val_accuracy: 0.9776\n",
      "Epoch 23/100\n",
      "97/97 [==============================] - 20s 204ms/step - loss: 0.2329 - accuracy: 0.9265 - val_loss: 0.0928 - val_accuracy: 0.9640\n",
      "Epoch 24/100\n",
      "97/97 [==============================] - 20s 206ms/step - loss: 0.2087 - accuracy: 0.9349 - val_loss: 0.0474 - val_accuracy: 0.9835\n",
      "Epoch 25/100\n",
      "97/97 [==============================] - 19s 200ms/step - loss: 0.1866 - accuracy: 0.9404 - val_loss: 0.0417 - val_accuracy: 0.9844\n",
      "Epoch 26/100\n",
      "97/97 [==============================] - 20s 205ms/step - loss: 0.1701 - accuracy: 0.9443 - val_loss: 0.0515 - val_accuracy: 0.9825\n",
      "Epoch 27/100\n",
      "97/97 [==============================] - 20s 208ms/step - loss: 0.2097 - accuracy: 0.9385 - val_loss: 0.0632 - val_accuracy: 0.9747\n",
      "Epoch 28/100\n",
      "97/97 [==============================] - 20s 208ms/step - loss: 0.1714 - accuracy: 0.9430 - val_loss: 0.0480 - val_accuracy: 0.9874\n",
      "Epoch 29/100\n",
      "97/97 [==============================] - 21s 211ms/step - loss: 0.1589 - accuracy: 0.9475 - val_loss: 0.0740 - val_accuracy: 0.9767\n",
      "Epoch 30/100\n",
      "97/97 [==============================] - 36s 361ms/step - loss: 0.1546 - accuracy: 0.9521 - val_loss: 0.0586 - val_accuracy: 0.9776\n",
      "Epoch 31/100\n",
      "97/97 [==============================] - 17s 171ms/step - loss: 0.1323 - accuracy: 0.9585 - val_loss: 0.0561 - val_accuracy: 0.9815\n",
      "Epoch 32/100\n",
      "97/97 [==============================] - 16s 169ms/step - loss: 0.1417 - accuracy: 0.9582 - val_loss: 0.0248 - val_accuracy: 0.9903\n",
      "Epoch 33/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.1346 - accuracy: 0.9602 - val_loss: 0.0330 - val_accuracy: 0.9912\n",
      "Epoch 34/100\n",
      "97/97 [==============================] - 17s 171ms/step - loss: 0.1741 - accuracy: 0.9453 - val_loss: 0.0363 - val_accuracy: 0.9893\n",
      "Epoch 35/100\n",
      "97/97 [==============================] - 16s 169ms/step - loss: 0.1534 - accuracy: 0.9524 - val_loss: 0.0305 - val_accuracy: 0.9893\n",
      "Epoch 36/100\n",
      "97/97 [==============================] - 17s 173ms/step - loss: 0.1433 - accuracy: 0.9540 - val_loss: 0.0276 - val_accuracy: 0.9903\n",
      "Epoch 37/100\n",
      "97/97 [==============================] - 17s 171ms/step - loss: 0.1132 - accuracy: 0.9637 - val_loss: 0.0244 - val_accuracy: 0.9893\n",
      "Epoch 38/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.1378 - accuracy: 0.9582 - val_loss: 0.0176 - val_accuracy: 0.9951\n",
      "Epoch 39/100\n",
      "97/97 [==============================] - 16s 169ms/step - loss: 0.1317 - accuracy: 0.9566 - val_loss: 0.0529 - val_accuracy: 0.9844\n",
      "Epoch 40/100\n",
      "97/97 [==============================] - 18s 180ms/step - loss: 0.1139 - accuracy: 0.9660 - val_loss: 0.0633 - val_accuracy: 0.9825\n",
      "Epoch 41/100\n",
      "97/97 [==============================] - 17s 171ms/step - loss: 0.1303 - accuracy: 0.9608 - val_loss: 0.0293 - val_accuracy: 0.9854\n",
      "Epoch 42/100\n",
      "97/97 [==============================] - 17s 170ms/step - loss: 0.1292 - accuracy: 0.9592 - val_loss: 0.0581 - val_accuracy: 0.9747\n",
      "Epoch 43/100\n",
      "97/97 [==============================] - 17s 172ms/step - loss: 0.1184 - accuracy: 0.9647 - val_loss: 0.0346 - val_accuracy: 0.9874\n",
      "Epoch 44/100\n",
      "97/97 [==============================] - 17s 180ms/step - loss: 0.1003 - accuracy: 0.9712 - val_loss: 0.0257 - val_accuracy: 0.9932\n",
      "Epoch 45/100\n",
      "97/97 [==============================] - 18s 180ms/step - loss: 0.1096 - accuracy: 0.9673 - val_loss: 0.0487 - val_accuracy: 0.9854\n",
      "Epoch 46/100\n",
      "97/97 [==============================] - 18s 180ms/step - loss: 0.0992 - accuracy: 0.9705 - val_loss: 0.0225 - val_accuracy: 0.9932\n",
      "Epoch 47/100\n",
      "97/97 [==============================] - 17s 173ms/step - loss: 0.1052 - accuracy: 0.9666 - val_loss: 0.0174 - val_accuracy: 0.9942\n",
      "Epoch 48/100\n",
      "97/97 [==============================] - 17s 173ms/step - loss: 0.1128 - accuracy: 0.9666 - val_loss: 0.0152 - val_accuracy: 0.9932\n",
      "Epoch 49/100\n",
      "97/97 [==============================] - 18s 182ms/step - loss: 0.0748 - accuracy: 0.9776 - val_loss: 0.0140 - val_accuracy: 0.9942\n",
      "Epoch 50/100\n",
      "97/97 [==============================] - 17s 172ms/step - loss: 0.0930 - accuracy: 0.9728 - val_loss: 0.0173 - val_accuracy: 0.9951\n",
      "Epoch 51/100\n",
      "97/97 [==============================] - 20s 210ms/step - loss: 0.0841 - accuracy: 0.9738 - val_loss: 0.0221 - val_accuracy: 0.9932\n",
      "Epoch 52/100\n",
      "97/97 [==============================] - 20s 206ms/step - loss: 0.0791 - accuracy: 0.9751 - val_loss: 0.0345 - val_accuracy: 0.9874\n",
      "Epoch 53/100\n",
      "97/97 [==============================] - 20s 205ms/step - loss: 0.1041 - accuracy: 0.9679 - val_loss: 0.0229 - val_accuracy: 0.9912\n",
      "Epoch 54/100\n",
      "97/97 [==============================] - 21s 213ms/step - loss: 0.0943 - accuracy: 0.9679 - val_loss: 0.0318 - val_accuracy: 0.9883\n",
      "Epoch 55/100\n",
      "97/97 [==============================] - 20s 204ms/step - loss: 0.0868 - accuracy: 0.9718 - val_loss: 0.0308 - val_accuracy: 0.9893\n",
      "Epoch 56/100\n",
      "97/97 [==============================] - 20s 201ms/step - loss: 0.0778 - accuracy: 0.9728 - val_loss: 0.0222 - val_accuracy: 0.9942\n",
      "Epoch 57/100\n",
      "97/97 [==============================] - 20s 202ms/step - loss: 0.0911 - accuracy: 0.9715 - val_loss: 0.0250 - val_accuracy: 0.9932\n",
      "Epoch 58/100\n",
      "97/97 [==============================] - 17s 173ms/step - loss: 0.1076 - accuracy: 0.9679 - val_loss: 0.0479 - val_accuracy: 0.9854\n",
      "Epoch 59/100\n",
      "97/97 [==============================] - 17s 173ms/step - loss: 0.0842 - accuracy: 0.9747 - val_loss: 0.0225 - val_accuracy: 0.9932\n"
     ]
    }
   ],
   "source": [
    "# Defining early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 10,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "CheckPoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'VGG16.h5' , save_best_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "     callbacks=[early_stopping , CheckPoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded166f1-1764-4345-9a5a-b17fa5bd5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = model.save('D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Model/VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a2891d-5f87-4790-ae3a-cdf7e3a2c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Model/VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ccee60c-4b0a-4fa1-91e3-10c7652f4f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 11s 348ms/step - loss: 0.0236 - accuracy: 0.9893\n",
      "Test accuracy: 0.9892578125\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = new_model.evaluate(test_generator, steps=test_step)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f3a6b-f705-48b5-89f3-1e5087b5e776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66866637-68d7-4368-9ac0-81640862f830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddb065-4814-49c3-8d9f-43571052e248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d9a66-c06d-4eb0-a0ca-3d75af828114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer Vision",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
