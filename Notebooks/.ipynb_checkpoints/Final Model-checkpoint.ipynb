{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88a95d16-1673-4c10-a8a8-b20cca3aaa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import h5py\n",
    "import cv2\n",
    "import datetime, os\n",
    "import string\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorboard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, MaxPooling2D, Dropout, SpatialDropout2D, BatchNormalization, Input,Activation, Dense, Flatten\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, save_model\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a722a979-b5a9-47a9-9c2f-316e85df1a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3087 images belonging to 7 classes.\n",
      "Found 1028 images belonging to 7 classes.\n",
      "Found 1028 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "## Set image dimensions and batch size\n",
    "img_height, img_width = 256, 256\n",
    "batch_size = 32\n",
    "\n",
    "# Data generators for training, validation, and testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2,  \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.3,  \n",
    "    horizontal_flip=True,\n",
    "    vertical_flip= True ,\n",
    "    fill_mode='nearest' \n",
    ")\n",
    "\n",
    "# No Augmentatin applied for both Validation and Testing Datasets\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Training',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # 'categorical' for multi-class classification, one-hot encoded vectors\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Validation',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Testing',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98582c2d-9b20-4a56-8295-1c495365ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step=train_generator.n//train_generator.batch_size\n",
    "test_step=test_generator.n//test_generator.batch_size\n",
    "valid_step =val_generator.n//val_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4e936b2-393b-4dfc-a744-4fa9fccc3946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 254, 254, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 127, 127, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 125, 125, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 62, 62, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 60, 60, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 30, 30, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 28, 28, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 14, 14, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 50176)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 256)               12845312  \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,235,527\n",
      "Trainable params: 13,235,527\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Wrapping our model and training in a function with the specified hyperparameters\n",
    "\n",
    "\n",
    "# Building the model\n",
    "model = Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_height,img_width, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(7, activation='softmax')\n",
    "])\n",
    "model.summary()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "270cf04e-371d-497c-8352-e78aaa17594f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Defining early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'auto',\n",
    "    min_delta = 0,\n",
    "    patience = 10,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ec818-80a4-47e3-9e86-5af847fca2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/96 [==============================] - 42s 432ms/step - loss: 0.4746 - accuracy: 0.8314 - val_loss: 0.2446 - val_accuracy: 0.9189\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 41s 430ms/step - loss: 0.4300 - accuracy: 0.8445 - val_loss: 0.2499 - val_accuracy: 0.9150\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 43s 444ms/step - loss: 0.4599 - accuracy: 0.8403 - val_loss: 0.3811 - val_accuracy: 0.8633\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 44s 461ms/step - loss: 0.4274 - accuracy: 0.8514 - val_loss: 0.2768 - val_accuracy: 0.9033\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 43s 445ms/step - loss: 0.3771 - accuracy: 0.8628 - val_loss: 0.2324 - val_accuracy: 0.9170\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 42s 440ms/step - loss: 0.3997 - accuracy: 0.8579 - val_loss: 0.2467 - val_accuracy: 0.9170\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 43s 450ms/step - loss: 0.3732 - accuracy: 0.8661 - val_loss: 0.2248 - val_accuracy: 0.9170\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 44s 459ms/step - loss: 0.3849 - accuracy: 0.8655 - val_loss: 0.2367 - val_accuracy: 0.9160\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 45s 467ms/step - loss: 0.3943 - accuracy: 0.8589 - val_loss: 0.2588 - val_accuracy: 0.9131\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 43s 449ms/step - loss: 0.4057 - accuracy: 0.8638 - val_loss: 0.2430 - val_accuracy: 0.9199\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 43s 447ms/step - loss: 0.3806 - accuracy: 0.8648 - val_loss: 0.2633 - val_accuracy: 0.8994\n",
      "Epoch 12/100\n",
      "12/96 [==>...........................] - ETA: 36s - loss: 0.3539 - accuracy: 0.8646"
     ]
    }
   ],
   "source": [
    "model.fit(train_generator,\n",
    "          steps_per_epoch=train_step,\n",
    "          epochs=100, \n",
    "          validation_data=val_generator,\n",
    "          validation_steps=valid_step,\n",
    "          callbacks=[ early_stopping]\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd91de-403d-4e90-8887-7c7793c1577a",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0a315-c050-4d9d-9718-3ab093821325",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = best_model.save('D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Model/Teeth.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf8951-aeb8-442c-8cc8-8824200e4a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loaded model is used for inference, evaluation, or further training\n",
    "new_model = keras.models.load_model('D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Model/Teeth.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a048a-66f1-4e97-bfe3-eb0732b7e998",
   "metadata": {},
   "source": [
    "### Evaluation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ffa49c-e05d-4c8b-8c10-0c85ff6d130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = new_model.evaluate(test_generator, steps=test_step)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a31a1-835a-46ff-a046-e3d4e6a5959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.samples , test_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ef967-8ce8-4fe8-a1ee-22ce5cfa7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get all the test samples and labels\n",
    "test_samples = []\n",
    "test_labels = []\n",
    "num_batches = test_generator.samples // test_generator.batch_size  # Total number of batches\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01832140-52d1-40fe-8392-e337768afaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(num_batches):\n",
    "    batch_samples, batch_labels = Test.next()\n",
    "    test_samples.append(batch_samples)\n",
    "    test_labels.append(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f55271-c3fc-4e5d-9d96-a1e29f386122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the samples and labels from all batches\n",
    "test_samples = np.concatenate(test_samples)\n",
    "test_labels = np.concatenate(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55badb2-c729-45fb-a0b3-48d57cc34210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test samples\n",
    "test_predictions = new_model.predict(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc1bb4-cf63-4470-9021-dc6ab7d89458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predicted class indices\n",
    "predicted_class_indices = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Calculate the true class indices\n",
    "true_class_indices = np.argmax(test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5084266-bb11-47a1-bf32-f72c18bdaa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "confusion_mat = confusion_matrix(true_class_indices, predicted_class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea124dbe-4de2-4d0b-82c8-4fc21edd0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d2b69-b2c8-433a-a4e2-b4c8f28cd1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation accuracy and loss\n",
    "plt.plot(model.history.history['accuracy'],label='training')\n",
    "plt.plot(model.history.history['val_accuracy'],label='validation')\n",
    "plt.plot(model.history.history[\"loss\"],label='loss')\n",
    "plt.plot(model.history.history[\"val_loss\"],label='val_loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4163b92-54a2-4abc-883b-de50d3c6ca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the test loss and accuracy\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5), sharey=False)\n",
    "fig.suptitle('Test Loss And Accuracy')\n",
    "\n",
    "sns.lineplot(ax=ax[0], data=best_model.history.history['val_loss'], color='r')\n",
    "ax[0].set_title('Loss')\n",
    "\n",
    "sns.lineplot(ax=ax[1], data=best_model.history.history['val_accuracy'], color='g')\n",
    "ax[1].set_title('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887664a1-11b6-4ae8-ad77-0647086ca234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the training and validation results\n",
    "def plot_results(model):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(best_model.history.history['accuracy'], label = 'train_accuracy')\n",
    "    plt.plot(best_model.history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(best_model.history.history['loss'], label = 'train_loss')\n",
    "    plt.plot(best_model.history.history['val_loss'], label = 'val_loss')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.suptitle('Train and Validation Results')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_results(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e60dd0-b258-4f59-9b0f-0c3d9370e36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer Vision",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
