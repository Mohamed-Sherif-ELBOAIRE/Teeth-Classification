{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4614ef8-cf9a-4821-b451-31b4ff7c6212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, MaxPooling2D, Dropout, SpatialDropout2D, BatchNormalization, Input,Activation, Dense, Flatten, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fc9be8f-3c1b-45c5-8e4e-d0864b952452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3087 images belonging to 7 classes.\n",
      "Found 1028 images belonging to 7 classes.\n",
      "Found 1028 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "img_height, img_width = 128, 128\n",
    "batch_size=32\n",
    "# Data generators for training, validation, and testing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # preprocessing_function=preprocess_input,  # Apply ResNet preprocessing\n",
    "    rescale=1./255, \n",
    "    rotation_range=45,\n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2,  \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.3,  \n",
    "    horizontal_flip=True,\n",
    "    vertical_flip= True ,\n",
    "    fill_mode='nearest' \n",
    ")\n",
    "# No Augmentation applied for both Validation and Testing Datasets\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Training',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # 'categorical' for multi-class classification, one-hot encoded vectors\n",
    ")\n",
    "\n",
    "val_generator = val_test_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Validation',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = val_test_datagen.flow_from_directory(\n",
    "    'D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Dataset/Teeth_Dataset/Testing',\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb681ff-bac9-421e-a1de-dea436c5283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 3591      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24,640,391\n",
      "Trainable params: 24,587,271\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resent50_pretrained = tf.keras.applications.ResNet50(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        input_shape=(224, 224,3),\n",
    "        pooling='avg',\n",
    "        classes=7,    \n",
    "    )\n",
    "for layer in resent50_pretrained.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "    \n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(resent50_pretrained)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "model.summary()vgg16_pretrained = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the layers you don't want to train\n",
    "for layer in vgg16_pretrained.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Alternatively, you can selectively unfreeze some layers like this:\n",
    "# for layer in vgg16_pretrained.layers[:15]:\n",
    "#     layer.trainable = False\n",
    "# for layer in vgg16_pretrained.layers[15:]:\n",
    "#     layer.trainable = True\n",
    "\n",
    "# Build your custom model architecture\n",
    "model = models.Sequential()\n",
    "\n",
    "# Add the pre-trained VGG16 base model\n",
    "model.add(vgg16_pretrained)\n",
    "\n",
    "# Add custom layers on top of the VGG16 base model\n",
    "model.add(layers.Flatten())  # Flatten the output from the VGG16 model\n",
    "model.add(layers.Dense(512, activation='relu'))  # Add a fully connected layer\n",
    "model.add(layers.Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(layers.Dense(256, activation='relu'))  # Add another fully connected layer\n",
    "model.add(layers.Dropout(0.5))  # Add another dropout layer\n",
    "model.add(layers.Dense(7, activation='softmax'))  # Final output layer with softmax (assuming 7 classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6533387-21b2-4db9-af24-4f499b8bfb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "113c739b-d899-4650-a2ed-1d2a0386c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "193/193 [==============================] - 103s 493ms/step - loss: 1.9883 - accuracy: 0.2446 - val_loss: 5.3633 - val_accuracy: 0.1167\n",
      "Epoch 2/100\n",
      "193/193 [==============================] - 90s 468ms/step - loss: 1.7627 - accuracy: 0.2958 - val_loss: 10.0368 - val_accuracy: 0.1449\n",
      "Epoch 3/100\n",
      "193/193 [==============================] - 92s 477ms/step - loss: 1.6256 - accuracy: 0.3699 - val_loss: 2.1428 - val_accuracy: 0.1887\n",
      "Epoch 4/100\n",
      "193/193 [==============================] - 91s 473ms/step - loss: 1.6209 - accuracy: 0.3648 - val_loss: 3.5746 - val_accuracy: 0.1449\n",
      "Epoch 5/100\n",
      "193/193 [==============================] - 93s 481ms/step - loss: 1.5332 - accuracy: 0.3946 - val_loss: 4.7004 - val_accuracy: 0.1372\n",
      "Epoch 6/100\n",
      "193/193 [==============================] - 92s 475ms/step - loss: 1.5537 - accuracy: 0.3907 - val_loss: 4.7682 - val_accuracy: 0.1518\n",
      "Epoch 7/100\n",
      "193/193 [==============================] - 94s 485ms/step - loss: 1.4822 - accuracy: 0.4312 - val_loss: 2.6020 - val_accuracy: 0.1926\n",
      "Epoch 8/100\n",
      " 44/193 [=====>........................] - ETA: 1:04 - loss: 1.4771 - accuracy: 0.4361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _EagerDefinedFunctionDeleter.__del__ at 0x0000022BD8B5B790>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Moham\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py\", line 305, in __del__\n",
      "    context.remove_function(self.name)\n",
      "  File \"C:\\Users\\Moham\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\context.py\", line 2740, in remove_function\n",
      "    context().remove_function(name)\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defining early stopping to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 10,\n",
    "    verbose = 0, \n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "     callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded166f1-1764-4345-9a5a-b17fa5bd5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = model.save('D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Model/Teeth.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2891d-5f87-4790-ae3a-cdf7e3a2c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model('D:/Mohamed Sheriff/Projects/Computer Vision Internship - Cellula Technologies/Teeth Classification/Model/Teeth.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccee60c-4b0a-4fa1-91e3-10c7652f4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy = new_model.evaluate(test_generator, steps=test_step)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e78e85-d131-427a-b502-886b2069ff42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f3a6b-f705-48b5-89f3-1e5087b5e776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66866637-68d7-4368-9ac0-81640862f830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbddb065-4814-49c3-8d9f-43571052e248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d9a66-c06d-4eb0-a0ca-3d75af828114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer Vision",
   "language": "python",
   "name": "cv_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
